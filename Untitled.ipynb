{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "test_news = '12年前爆發的花蓮縣壽豐鄉鯉魚潭風景區的「綠湖國際大飯店」開發弊案 ，業者王桂霜為讓計畫案順利進行，以250萬元行賄時任內政部都市計畫委員會委員李威儀與其學生藍秀琪；經法院12年審理，2017年12月花蓮高分院更三審判李威儀12年徒刑、藍秀琪9年，皆褫奪公權6年，王桂霜1年9月、褫奪公權1年；最高法院日前駁回上訴，全案全部定讞。此開發計畫在花蓮縣都市計畫委員會審查時，曾5度闖關未過，1999年起死回生，業者雙聯投資實業公司董事長王桂霜，申請將鯉魚潭的22公頃公園等用地，變更為旅館用地與停車場，隔年8月花蓮縣政府通過變更計畫案，函送內政部都市計畫委員會審議。請繼續往下閱讀...都委會專案小組赴現場勘查後，前3次專案會議都明確表達反對之意，強烈質疑公園綠地面積只占6.4%，未達10%的最低法定下限，更何況是風景特定區，認為不宜輕率將綠地變更為建地。王桂霜為求解套，透過花蓮縣政府承辦技士居中介紹，與負責審查此案的的時任內政部都委會專案小組召集人、台科大建築系副教授李威儀搭上線，相約在第4次專案會議的前1天，於台北市西華飯店碰面。李威儀在西華當面向王桂霜提議，可利用他的碩士在職專班學生藍秀琪當「白手套」，由藍秀琪開設的公司來負責規劃開發計畫書，李向王索賄250萬元，分3期付清。李與藍虛構「綠湖觀光飯店規劃案」，試圖以規劃設計費的名義讓賄款合法化，李並教唆都委會2名承辦人登載不實資料，企圖矇混過關。幾經運作，李威儀趕在他的委員任期屆滿之前，於2001年5月、6月連開最後2次專案小組會議，做成「尊重花蓮縣府意見，建議本案原則同意變更」的審查意見，送內政部審議，並在卸任後列席委員會報告，企圖說服與會人士支持。調查局東機站事後獲報，報告花蓮地檢署指揮偵辦，2007年起訴李威儀等人。案經法院審理12年，李威儀等人一度獲判無罪，但被撤銷發回，更三審依違背職務收賄罪，判李、藍各12年、9年徒刑，王桂霜以行賄罪判刑1年9月。最高法院認定更三審判決沒有違誤， 駁回上訴確定。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.callbacks\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Bidirectional, LSTM, Dense\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from datetime import datetime\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 變數區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'C:\\Users\\jason\\Desktop\\python\\01_project\\01_esun\\api\\data'\n",
    "config_path = os.path.join(model_path, r'bert\\bert_config.json')\n",
    "checkpoint_path = os.path.join(model_path, r'bert\\bert_model.ckpt')\n",
    "dict_path = os.path.join(model_path, r'bert\\vocab.txt')\n",
    "bert_LSTM_model_path = os.path.join(model_path, r'aml_model_weight.h5')\n",
    "ner_model_path = os.path.join(model_path, r'ner_model_weight.h5')\n",
    "aml_model_2_path = os.path.join(model_path, r'aml_model_2.h5')\n",
    "maxlen = 256\n",
    "maxlen_ner = 512\n",
    "input_shape = (maxlen_ner, )\n",
    "maxlen_sentences = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create model & load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_LSTM_model():\n",
    "    \n",
    "    model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen)\n",
    "    sequence_output = model.layers[-9].output\n",
    "    sequence_output = Bidirectional(LSTM(128, return_sequences=False))(sequence_output)\n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model = Model(model.input, output)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_BiLSTM_CRF_model():\n",
    "    \n",
    "    ner_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_ner)\n",
    "    bert_output = ner_model.layers[-9].output\n",
    "    X = Lambda(lambda x: x[:, 0: input_shape[0]])(bert_output)\n",
    "    X = Bidirectional(LSTM(128, return_sequences=True))(X)\n",
    "    #X = TimeDistributed(Dense(len(y_token_dict), activation='relu'))(X)\n",
    "    output = CRF(3, sparse_target = True)(X)    \n",
    "    ner_model = Model(ner_model.input, output)\n",
    "    \n",
    "    for layer in ner_model.layers:\n",
    "        layer.trainable = False\n",
    "    ner_model.layers[-1].trainable = True\n",
    "    ner_model.layers[-2].trainable = True\n",
    "    \n",
    "    return ner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_LSTM_model2():\n",
    "    \n",
    "    model2 = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True, seq_len=maxlen_sentences)\n",
    "    sequence_output = model2.layers[-9].output\n",
    "    #sequence_output = Lambda(lambda x: x[:, 0])(sequence_output)\n",
    "    sequence_output = Bidirectional(LSTM(128, return_sequences=False))(sequence_output)\n",
    "    output = Dense(1, activation='sigmoid')(sequence_output)\n",
    "    model2 = Model(model2.input, output)\n",
    "    \n",
    "    for layer in model2.layers:\n",
    "        layer.trainable = False\n",
    "    model2.layers[-1].trainable = True\n",
    "    model2.layers[-2].trainable = True\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jason\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\jason\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = bert_LSTM_model()\n",
    "model.load_weights(bert_LSTM_model_path)\n",
    "ner_model = bert_BiLSTM_CRF_model()\n",
    "ner_model.load_weights(ner_model_path)\n",
    "model2 = bert_LSTM_model2()\n",
    "model2.save_weights(aml_model_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_marks(content):\n",
    "    content = re.sub('<[^>]*>|【[^】]*】|（[^）]*）|〔[^〕]*〕', '', content)\n",
    "    content = content.replace('記者', '＜') \\\n",
    "                     .replace('報導', '＞') \\\n",
    "                     .replace('▲', '') \\\n",
    "                     .replace('。　', '。') \\\n",
    "                     .replace('\b', '') \\\n",
    "                     .replace('.', '') \\\n",
    "                     .replace(' ', '') \\\n",
    "                     .replace('“', '「') \\\n",
    "                     .replace('”', '」')\n",
    "    content = re.sub('＜[^＞]*＞', '', content)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(dict_path):\n",
    "    \n",
    "    token_dict = {}\n",
    "    with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "    tokenizer = Tokenizer(token_dict)\n",
    "    \n",
    "    return tokenizer, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded(tokenizer, data, maxlen):\n",
    "    \n",
    "    x, y, z = [], [], []\n",
    "    x1, x2 = tokenizer.encode(data, max_len=maxlen)\n",
    "    x3 = list(map(lambda x: 1 if x != 0 else 0, x1))\n",
    "    x.append(x1)\n",
    "    y.append(x2)\n",
    "    z.append(x3)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    data = [x, y, z]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把大於512的新聞以句點分段 (bert最多只能吃512)\n",
    "def split_content(content):\n",
    "\n",
    "    if (len(content) > 512) & (len(content) <= 1024):\n",
    "\n",
    "        s_split = [(i, abs(len(content)//2 - content.find(x)), x) for i, x in enumerate(content.split('。'))]\n",
    "        idx_left = min(s_split, key=lambda x: x[1])[0]\n",
    "        first = \"。\".join([s_split[i][2] for i in range(idx_left)])\n",
    "        second = \"。\".join([s_split[i][2] for i in range(idx_left, len(s_split))])    \n",
    "        contents = [first, second]\n",
    "        \n",
    "        return contents\n",
    "\n",
    "    elif len(content) > 1024:\n",
    "\n",
    "        s_split1 = [(i, abs(len(content)//3 - content.find(x)), x) for i, x in enumerate(content.split('。'))]\n",
    "        s_split2 = [(i, abs(len(content)*2//3 - content.find(x)), x) for i, x in enumerate(content.split('。'))]\n",
    "        idx_left1 = min(s_split1, key=lambda x: x[1])[0]\n",
    "        idx_left2 = min(s_split2, key=lambda x: x[1])[0]\n",
    "        first = \"。\".join([s_split1[i][2] for i in range(idx_left1)])\n",
    "        second = \"。\".join([s_split1[i][2] for i in range(idx_left1, idx_left2)])\n",
    "        third = \"。\".join([s_split1[i][2] for i in range(idx_left2, len(s_split1))])\n",
    "        contents = [first, second, third]\n",
    "        \n",
    "        return contents\n",
    "    \n",
    "    else:\n",
    "        return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentance_list(test_news:str, name_list:list) -> list:\n",
    "    \"\"\"@test_news: 原文\n",
    "       @name_list: ner抓出的人名\n",
    "       @return: sentance_list, 用於下一階段\n",
    "    \"\"\"\n",
    "    sentance_list = []\n",
    "    # 原人名list排序 不知道為啥\n",
    "    name_list.sort(reverse=True)\n",
    "\n",
    "    # 把人名從list提出\n",
    "    for name in name_list:    \n",
    "        #建立tmp list, 刪掉除了自己\n",
    "        tmp_name_list = name_list.copy()\n",
    "        tmp_name_list.remove(name)\n",
    "        # 切分句子, 並將自己以外的人取代為 \"其他人名\"\n",
    "        test_split_content = test_news\n",
    "        test_split_content = re.sub('|'.join(tmp_name_list), '其他人名', test_split_content)\n",
    "        test_split_content = test_split_content.replace('。','=。').replace('，','*，').replace('？','+？')\n",
    "        test_split_content = re.split('，|。|？', test_split_content)\n",
    "        test_split_content = list(map(lambda x: x.replace('=','。').replace('*','，').replace('+','？'), test_split_content))\n",
    "        test_split_content = list(filter(None, test_split_content))\n",
    "        # 建立sentances list, 即上拼接下文\n",
    "        for i, s in enumerate(test_split_content):\n",
    "            sentance = ''\n",
    "            if name in s:\n",
    "                # 正常有前後文共3句的情況\n",
    "                # 第1句出現句號 -> 拼2+3\n",
    "                # 第2句出現句號 -> 拼1+2\n",
    "                # 第3句或沒有句號 -> 拚1+2+3\n",
    "                try:\n",
    "                    start = test_split_content[i-1]\n",
    "                    mid = test_split_content[i]\n",
    "                    end = test_split_content[i+1]\n",
    "                    if start.find('。') > 0:\n",
    "                        sentance = mid + end\n",
    "                    elif mid.find('。') > 0:\n",
    "                        sentance = start + mid\n",
    "                    else:\n",
    "                        sentance = start + mid + end\n",
    "                except:\n",
    "                    # 若名字出現在頭尾的例外處理\n",
    "                    try:             \n",
    "                        start = test_split_content[i-1]\n",
    "                        mid = test_split_content[i]\n",
    "                        if start.find('。') > 0:\n",
    "                            sentance = mid \n",
    "                        else:\n",
    "                            sentance = start + mid \n",
    "                    except:               \n",
    "                        mid = test_split_content[i]\n",
    "                        end = test_split_content[i+1]\n",
    "                        if mid.find('。') > 0:\n",
    "                            sentance = mid\n",
    "                        else:\n",
    "                            sentance = mid + end\n",
    "                sentance_list.append((sentance, name))\n",
    "                \n",
    "    return sentance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(sentance_list:list) -> pd.DataFrame:\n",
    "    AML = pd.DataFrame(sentance_list, columns=['sentance', 'name'])\n",
    "    name_list = []\n",
    "    full_name = [s[1] for s in sentance_list]\n",
    "    full_3name = [s[1] for s in sentance_list if len(s[1]) == 3]\n",
    "    a = Counter([name[0] for name in full_3name])\n",
    "    keep = [k for k,v in a.items() if v == 1]\n",
    "    full_3name_filter = [name for name in full_3name if name[0] in keep]\n",
    "    name_dict = dict((name[0], name) for name in full_3name_filter)   # ex: {'陳' : '陳水扁'}\n",
    "    name_dict_2 = dict(zip([name[0:2] for name in full_3name], full_3name))  # ex: {'王音': '王音之'}\n",
    "    for name in full_name:\n",
    "        if (name[0] in name_dict.keys()) & (len(name) == 1):\n",
    "            name_list.append(name_dict.get(name[0]))\n",
    "        elif (name[0] in name_dict.keys()) & (len(name) == 2) & (name[-1] in ['男', '嫌', '婦', '夫', '某', '女', '妻',\\\n",
    "                                                                              '員', '稱', '家', '哥', '媽', '生', '處',\\\n",
    "                                                                              '和', '揆', '要', '再', '董', '涉', '母',\\\n",
    "                                                                              '辱', '公', '少', '為', '指', '翁', '粉',\\\n",
    "                                                                              '趁', '仔', '依', '氏', '父']):\n",
    "            name_list.append(name_dict.get(name[0]))\n",
    "        elif (name in name_dict_2.keys()) & (len(name) == 2):\n",
    "            name_list.append(name_dict_2.get(name))\n",
    "        else:\n",
    "            name_list.append(name)\n",
    "    # 排除重複資料、排除一字、兩字簡稱、兩字三字四字姓不在姓氏表中的人\n",
    "    AML['name'] = name_list\n",
    "    AML = AML.drop_duplicates()\n",
    "    AML = AML[AML['name'].apply(lambda x: (len(x) > 1) )]\n",
    "    AML = AML[~AML['name'].apply(lambda x: (len(x) == 2) & (x[1] in ['男', '嫌', '婦', '夫', '某', '女', '妻',\\\n",
    "                                                                     '員', '稱', '家', '哥', '媽', '生', '處',\\\n",
    "                                                                     '和', '揆', '要', '再', '董', '涉', '母',\\\n",
    "                                                                     '辱', '公', '少', '為', '指', '翁', '粉',\\\n",
    "                                                                     '趁', '仔', '依', '氏', '父']))]\n",
    "    return AML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_aml(model, data, aml_threshold):\n",
    "    \n",
    "    #第一階段預測，大於aml_threshold者為疑似aml文章   \n",
    "    prediction = model.predict(data)\n",
    "    prediction[prediction >= aml_threshold] = 1\n",
    "    prediction[prediction < aml_threshold] = 0\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得名字 (預測結果為onehot的狀態)\n",
    "def get_name(input_id, y_pred, token_dict):\n",
    "    \n",
    "    label_list = []\n",
    "    word_dict = {v: k for k, v in token_dict.items()}\n",
    "    \n",
    "    for input_data, y in zip(input_id, y_pred):\n",
    "        people_index = ''.join([str(a) for a in list(y)])\n",
    "        j = 0\n",
    "        name_list = []\n",
    "        split_index = re.findall('[12]2*', people_index)\n",
    "        name = ''.join([word_dict.get(input_data[index]) for index, value in enumerate(y) if value != 0])\n",
    "        \n",
    "        # [UNK], [PAD]會被算成 5 個字元，避免轉換成文字的index因長度不同對不上，故用 1 個字元的其他符號替代\n",
    "        # 王春甡 -> 王春[UNK] -> 王春?\n",
    "        name = name.replace('[UNK]','?')\n",
    "        name = name.replace('[PAD]','!')\n",
    "        \n",
    "        for i in split_index:\n",
    "            name_list.append(name[0+j:len(i)+j])\n",
    "            j = len(i) + j\n",
    "            \n",
    "        name_list = [name for name in name_list]\n",
    "        label_list.append(list(set(name_list)))\n",
    "    \n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "# 注意修改維度\n",
    "tokenizer, token_dict = create_tokenizer(dict_path=dict_path)\n",
    "test_news = clean_marks(test_news)\n",
    "data = encoded(tokenizer=tokenizer, data=test_news, maxlen=maxlen)\n",
    "prediction = predict_aml(model, data=data, aml_threshold=0.4)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0][0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['王', '藍秀琪', '李', '藍', '王桂霜', '李威儀']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ner = split_content(test_news)\n",
    "name_list = []\n",
    "for i in range(len(test_ner)):\n",
    "    input_id, segment_id, mask_input = encoded(tokenizer, test_ner[i], maxlen=maxlen_ner)\n",
    "    prediction = ner_model.predict([input_id, segment_id, mask_input])\n",
    "    y_pred = np.argmax(prediction, axis=-1)\n",
    "    tmp_list = get_name(input_id, y_pred, token_dict)[0]\n",
    "    name_list.extend(tmp_list)\n",
    "name_list = list(set(name_list))\n",
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentance_list = create_sentance_list(test_news, name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('其他人名與藍虛構「綠湖觀光飯店規劃案」，試圖以規劃設計費的名義讓賄款合法化，', '藍'),\n",
       " ('更三審依違背職務收賄罪，判其他人名、藍各12年、9年徒刑，其他人名以行賄罪判刑1年9月。', '藍'),\n",
       " ('由其他人名開設的公司來負責規劃開發計畫書，其他人名向王索賄250萬元，分3期付清。', '王'),\n",
       " ('由其他人名開設的公司來負責規劃開發計畫書，李向其他人名索賄250萬元，分3期付清。', '李'),\n",
       " ('李與其他人名虛構「綠湖觀光飯店規劃案」，試圖以規劃設計費的名義讓賄款合法化，', '李'),\n",
       " ('試圖以規劃設計費的名義讓賄款合法化，李並教唆都委會2名承辦人登載不實資料，企圖矇混過關。', '李'),\n",
       " ('更三審依違背職務收賄罪，判李、其他人名各12年、9年徒刑，其他人名以行賄罪判刑1年9月。', '李')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentance_list = create_sentance_list(test_news, name_list)\n",
    "AML = create_dataset(sentance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
